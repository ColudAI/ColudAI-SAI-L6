import torch
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm
from config import TrainingConfig, logger
from model import TransformerModel
from data import prepare_dataloaders

def data_preparation(batch, cfg): 
    """
    å‡†å¤‡è¾“å…¥æ•°æ®ï¼ŒåŒ…æ‹¬ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å’Œæå–å¿…è¦çš„è¾“å…¥è¾“å‡ºæ•°æ®
    """
    inputs = batch["input_ids"].to(cfg.DEVICE, non_blocking=True)
    labels = batch["labels"].to(cfg.DEVICE, non_blocking=True)
    dec_input = inputs[:, :-1]
    dec_output = labels[:, 1:]
    return inputs, dec_input, dec_output

def forward_pass(model, inputs, dec_input, dec_output, criterion, cfg): 
    """
    æ‰§è¡Œæ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼ŒåŒ…æ‹¬æ··åˆç²¾åº¦è®¡ç®—
    """
    with autocast(device_type=cfg.DEVICE.type, enabled=cfg.USE_AMP): 
        outputs, balance_loss = model(inputs, dec_input)
        loss = criterion(outputs.reshape(-1, outputs.size(-1)), dec_output.reshape(-1))
        total_loss = (loss + balance_loss * cfg.BALANCE_COEF) / cfg.GRAD_ACCUM
    return total_loss

def backward_and_update(optimizer, scaler, model, total_loss, cfg, step, train_loader): 
    """
    æ‰§è¡Œåå‘ä¼ æ’­ã€æ¢¯åº¦è£å‰ªå’Œå‚æ•°æ›´æ–°
    """
    scaler.scale(total_loss).backward()
    if step % cfg.GRAD_ACCUM == 0 or step == len(train_loader): 
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.CLIP_GRAD_NORM)
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad(set_to_none=True)

def train_epoch(
    model: TransformerModel, 
    train_loader: torch.utils.data.DataLoader, 
    optimizer: torch.optim.Optimizer, 
    scheduler: torch.optim.lr_scheduler.LambdaLR, 
    criterion: torch.nn.Module, 
    scaler: GradScaler, 
    epoch: int
) -> float: 
    """
    æ‰§è¡Œå•ä¸ªè®­ç»ƒå‘¨æœŸ

    å‚æ•°: 
        model: å¾…è®­ç»ƒæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨å®ä¾‹
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        criterion: æŸå¤±å‡½æ•°
        scaler: æ··åˆç²¾åº¦æ¢¯åº¦ç¼©æ”¾å™¨
        epoch: å½“å‰å‘¨æœŸæ•°

    è¿”å›: 
        avg_loss: å¹³å‡è®­ç»ƒæŸå¤±
    """
    cfg = TrainingConfig()
    model.train()
    optimizer.zero_grad(set_to_none=True)
    total_loss = 0.0

    pbar = tqdm(train_loader, 
                desc=f"ğŸš€ è®­ç»ƒå‘¨æœŸ {epoch + 1}/{cfg.EPOCHS}", 
                dynamic_ncols=True)

    for step, batch in enumerate(pbar, 1): 
        # æ•°æ®å‡†å¤‡
        inputs, dec_input, dec_output = data_preparation(batch, cfg)

        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        loss = forward_pass(model, inputs, dec_input, dec_output, criterion, cfg)

        # åå‘ä¼ æ’­ä¸æ¢¯åº¦è£å‰ª
        backward_and_update(optimizer, scaler, model, loss, cfg, step, train_loader)

        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler.step()

        # ç´¯ç§¯æŸå¤±
        total_loss += loss.item() * cfg.GRAD_ACCUM

    avg_loss = total_loss / len(train_loader.dataset)
    return avg_loss