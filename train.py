#        â”â”“ã€€ã€€ã€€â”â”“+ +
#ã€€ã€€ã€€â”â”›â”»â”â”â”â”›â”»â”“ + +
#ã€€ã€€ã€€â”ƒã€€ã€€ã€€ã€€ã€€ã€€ã€€â”ƒ ã€€
#ã€€ã€€ã€€â”ƒã€€ã€€ã€€â”ã€€ã€€ã€€â”ƒ ++ + + +
#ã€€ã€€ â–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆ â”ƒ+
#ã€€ã€€ã€€â”ƒã€€ã€€ã€€ã€€ã€€ã€€ã€€â”ƒ +
#ã€€ã€€ã€€â”ƒã€€ã€€ã€€â”»ã€€ã€€ã€€â”ƒ
#ã€€ã€€ã€€â”ƒã€€ã€€ã€€ã€€ã€€ã€€ã€€â”ƒ + +
#ã€€ã€€ã€€â”—â”â”“ã€€ã€€ã€€â”â”â”›
#ã€€ã€€ã€€ã€€ã€€â”ƒã€€ã€€ã€€â”ƒã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
#ã€€ã€€ã€€ã€€ã€€â”ƒã€€ã€€ã€€â”ƒ + + + +
#ã€€ã€€ã€€ã€€ã€€â”ƒã€€ã€€ã€€â”ƒã€€ã€€ã€€ã€€Codes are far away from bugs with the animal protectingã€€ã€€ã€€
#ã€€ã€€ã€€ã€€ã€€â”ƒã€€ã€€ã€€â”ƒ + ã€€ã€€ã€€ã€€ç¥å…½ä¿ä½‘,ä»£ç æ°¸æ— bugåŠæŠ¥é”™
#ã€€ã€€ã€€ã€€ã€€â”ƒã€€ã€€ã€€â”ƒ
#ã€€ã€€ã€€ã€€ã€€â”ƒã€€ã€€ã€€â”ƒã€€ã€€+ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
#ã€€ã€€ã€€ã€€ã€€â”ƒã€€ ã€€ã€€â”—â”â”â”â”“ + +
#ã€€ã€€ã€€ã€€ã€€â”ƒ ã€€ã€€ã€€ã€€ã€€ã€€ã€€â”£â”“
#ã€€ã€€ã€€ã€€ã€€â”ƒ ã€€ã€€ã€€ã€€ã€€ã€€ã€€â”â”›
#ã€€ã€€ã€€ã€€ã€€â”—â”“â”“â”â”â”³â”“â”â”› + + + +
#ã€€ã€€ã€€ã€€ã€€ã€€â”ƒâ”«â”«ã€€â”ƒâ”«â”«
#ã€€ã€€ã€€ã€€ã€€ã€€â”—â”»â”›ã€€â”—â”»â”›+ + + +
import torch
from torch.cuda.amp import GradScaler, autocast
from torch.utils.data import DataLoader
from tqdm import tqdm
from pathlib import Path

# ç»Ÿä¸€é…ç½®ç®¡ç†
from config import CONFIG, LOGGER
from model import TransformerModel
from data import DataLoaderPreparer

def data_preparation(batch: dict) -> dict:
    device = CONFIG.DEVICE
    inputs = batch["input_ids"].to(device, non_blocking=True)

    # æ˜¾å¼å¯¹é½æ‰€æœ‰è¾“å‡ºè®¾å¤‡
    return {
        "inputs": inputs.contiguous(),
        "dec_input": inputs[:, :-1].contiguous(),
        "dec_output": inputs[:, 1:].contiguous()
    }

def forward_pass(model: TransformerModel, batch_data: dict, criterion: torch.nn.Module) -> torch.Tensor:
    """ä¼˜åŒ–åçš„å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨é…ç½®ç¼“å­˜"""
    # ç¼“å­˜å¸¸ç”¨é…ç½®å‚æ•°
    amp_enabled = CONFIG.USE_AMP
    amp_dtype = torch.bfloat16 if CONFIG.USE_BF16 else torch.float16
    balance_coef = CONFIG.BALANCE_COEF
    grad_accum = CONFIG.GRAD_ACCUM
    device = CONFIG.DEVICE  # è·å–è®¾å¤‡

    # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    inputs = batch_data["inputs"].to(device)
    dec_input = batch_data["dec_input"].to(device)
    dec_output = batch_data["dec_output"].to(device)

    with autocast(enabled=amp_enabled, dtype=amp_dtype):
        outputs, balance_loss = model(inputs, dec_input)
        logits = outputs.view(-1, outputs.size(-1))
        targets = dec_output.view(-1)

        # æ£€æŸ¥å¹¶è£å‰ªç›®æ ‡å€¼
        print(f"Targets max: {targets.max()}, min: {targets.min()}, vocab_size: {model.vocab_size}")
        targets = torch.clamp(targets, 0, model.vocab_size - 1)  # è£å‰ªç›®æ ‡å€¼

        loss_mask = targets != -100
        loss = criterion(logits[loss_mask], targets[loss_mask])
        return (loss + balance_loss * balance_coef) / grad_accum

def backward_and_update(
        optimizer: torch.optim.Optimizer,
        scaler: GradScaler,
        total_loss: torch.Tensor,
        step: int,
        total_steps: int
):
    """ä¼˜åŒ–åçš„åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°"""
    grad_accum = CONFIG.GRAD_ACCUM
    clip_grad_norm = CONFIG.CLIP_GRAD_NORM
    debug_mode = CONFIG.DEBUG

    scaler.scale(total_loss).backward()

    if (step + 1) % grad_accum == 0 or (step + 1) == total_steps:
        scaler.unscale_(optimizer)

        # æ¢¯åº¦è£å‰ªä¼˜åŒ–
        for group in optimizer.param_groups:
            if group.get("no_clip", False):
                continue
            torch.nn.utils.clip_grad_norm_(
                group["params"],
                group["lr"],  # æ·»åŠ  lr å‚æ•°
                error_if_nonfinite=debug_mode
            )

        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad(set_to_none=True)


def train_epoch(
        model: TransformerModel,
        train_loader: DataLoader,
        optimizer: torch.optim.Optimizer,
        scheduler: torch.optim.lr_scheduler.LambdaLR,
        criterion: torch.nn.Module,
        scaler: GradScaler,
        epoch: int
) -> float:
    """ä¼˜åŒ–åçš„è®­ç»ƒå‘¨æœŸï¼Œæ”¹è¿›è¿›åº¦æ˜¾ç¤ºå’Œå†…å­˜ç®¡ç†"""
    model.train()
    total_loss = 0.0
    accumulated_steps = 0

    # ç¼“å­˜é…ç½®å‚æ•°
    grad_accum = CONFIG.GRAD_ACCUM
    total_epochs = CONFIG.EPOCHS
    warmup_steps = CONFIG.WARMUP_STEPS

    with tqdm(
            enumerate(train_loader),
            total=len(train_loader),
            desc=f"ğŸš€ è®­ç»ƒå‘¨æœŸ [{epoch + 1}/{total_epochs}]",
            bar_format="{l_bar}{bar:20}{r_bar}{bar:-20b}"
    ) as pbar:
        for step, batch in pbar:
            try:
                batch_data = data_preparation(batch)
                loss = forward_pass(model, batch_data, criterion)

                backward_and_update(
                    optimizer=optimizer,
                    scaler=scaler,
                    total_loss=loss,
                    step=step,
                    total_steps=len(train_loader)
                )

                # å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–
                if (step + 1) % grad_accum == 0:
                    scheduler.step()

                total_loss += loss.item() * grad_accum
                accumulated_steps += 1

                # å®æ—¶ç›‘æ§æŒ‡æ ‡æ›´æ–°
                current_lr = optimizer.param_groups[0]["lr"]
                mem_usage = torch.cuda.memory_allocated() // 1024 ** 2 if torch.cuda.is_available() else 0
                pbar.set_postfix({
                    "loss": f"{total_loss / accumulated_steps:.3f}",
                    "lr": f"{current_lr:.2e}",
                    "gpu_mem": f"{mem_usage}MB"
                })

            except RuntimeError as e:
                if "CUDA out of memory" in str(e):
                    LOGGER.error("æ˜¾å­˜ä¸è¶³ï¼Œå°è¯•å‡å°æ‰¹æ¬¡å¤§å°æˆ–å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
                    torch.cuda.empty_cache()
                    continue
                raise

    avg_loss = total_loss / accumulated_steps if accumulated_steps else 0.0
    LOGGER.info(f"è®­ç»ƒå‘¨æœŸ {epoch + 1} å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.4f}")
    return avg_loss


def setup_optimizer(model: TransformerModel) -> torch.optim.Optimizer:
    """ä¼˜åŒ–å™¨é…ç½®æ”¹è¿›"""
    return torch.optim.AdamW(
        model.parameters(),
        lr=CONFIG.BASE_LR,
        eps=1e-8,
        weight_decay=CONFIG.WEIGHT_DECAY
    )


def setup_scheduler(optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.LambdaLR:
    """å­¦ä¹ ç‡è°ƒåº¦å™¨ä¼˜åŒ–"""
    warmup_steps = CONFIG.WARMUP_STEPS
    return torch.optim.lr_scheduler.LambdaLR(
        optimizer,
        lr_lambda=lambda step: min((step + 1) / warmup_steps, 1.0)
    )


def main():
    """ä¸»å‡½æ•°ä¼˜åŒ–ï¼Œæ”¹è¿›åˆå§‹åŒ–æµç¨‹"""
    # è®¾å¤‡é€‰æ‹©
    device = CONFIG.DEVICE
    CONFIG.DEVICE = device

    # æ¨¡å‹åˆå§‹åŒ–
    model = TransformerModel(
        vocab_size=CONFIG.VOCAB_SIZE,
        embed_size=CONFIG.EMBED_SIZE,
        num_heads=CONFIG.NUM_HEADS,
        hidden_dim=CONFIG.HIDDEN_DIM,
        enc_layers=CONFIG.ENC_LAYERS,
        dec_layers=CONFIG.DEC_LAYERS,
        dropout=CONFIG.DROPOUT,
        use_moe=CONFIG.USE_MOE,
        num_experts=CONFIG.NUM_EXPERTS,
        expert_dim=CONFIG.EXPERT_DIM,
        device=device
    )

    print(f"æ¨¡å‹æœ€ç»ˆè®¾å¤‡: {next(model.parameters()).device}")
    print(f"Embeddingå±‚æƒé‡è®¾å¤‡: {model.embed.weight.device}")

    # æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–
    dl_preparer = DataLoaderPreparer()
    train_loader, val_loader = dl_preparer.prepare()

    # æ£€æŸ¥ç‚¹æ¢å¤
    if CONFIG.MODEL_SAVE_PATH.exists():
        model.load_state_dict(torch.load(CONFIG.MODEL_SAVE_PATH, map_location=device))
        LOGGER.info(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹: {CONFIG.MODEL_SAVE_PATH}")

    # è®­ç»ƒç»„ä»¶åˆå§‹åŒ–
    optimizer = setup_optimizer(model)
    scheduler = setup_scheduler(optimizer)
    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)
    scaler = GradScaler(enabled=CONFIG.USE_AMP)

    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    CONFIG.MODEL_SAVE_PATH.parent.mkdir(parents=True, exist_ok=True)

    # è®­ç»ƒå¾ªç¯
    for epoch in range(CONFIG.EPOCHS):
        avg_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion, scaler, epoch)
        checkpoint_path = CONFIG.MODEL_SAVE_PATH.parent / f"model_epoch_{epoch + 1}.pth"
        torch.save(model.state_dict(), checkpoint_path)
        LOGGER.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³: {checkpoint_path}")


if __name__ == "__main__":
    main()